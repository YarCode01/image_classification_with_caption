{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davide = \"loser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import time\n",
    "import random \n",
    "import json\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"M1 pro GPU is activated\")\n",
    "    import os\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '0'\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n",
    "    cuda_id = torch.cuda.current_device()\n",
    "    print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "        \n",
    "    print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the images already resized 250 x 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.csv') as fp:\n",
    "    # read a list of lines into data\n",
    "    data = fp.readlines()\n",
    "\n",
    "data[4790] = data[4790].replace(\"/\", \"\")\n",
    "data[14716] = data[14716].replace(\"/\", \"\")\n",
    "data[14961] = data[14961].replace(\"/\", \"\")\n",
    "data[29895] = data[29895].replace(\"/\", \"\")\n",
    "\n",
    "# and write everything back\n",
    "with open('train.csv', 'w') as file:\n",
    "    file.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "FILENAME = 'train.csv'\n",
    "with open(FILENAME) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    df_train = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "FILENAME = 'test.csv'\n",
    "with open(FILENAME) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    df_test = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_train.shape[0]):\n",
    "    df_train.Labels[i] = [int(j) for j in df_train.Labels[i].split()]\n",
    "max_i = 0\n",
    "for i in df_train.Labels:\n",
    "    max_i = max(max_i, max(i))\n",
    "min_i = 19\n",
    "for i in df_train.Labels:\n",
    "    min_i = min(min_i, min(i))\n",
    "min_i\n",
    "\n",
    "for i in range(1, max_i+1):\n",
    "    df_train[f'{i}'] = 0\n",
    "for i in range(df_train.shape[0]):\n",
    "    for j in df_train.Labels[i]:\n",
    "        df_train[f\"{j}\"][i] = 1\n",
    "df_train.to_csv(\"df_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_train.shape[0]):\n",
    "    if f\"{i}.jpg\" != df_train.loc[i,'ImageID']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"df_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train['Labels'].explode()\n",
    "\n",
    "# Plot the distribution of labels using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts = labels.value_counts()\n",
    "label_counts.plot(kind='bar')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Labels')\n",
    "\n",
    "# Set custom x-tick labels\n",
    "plt.xticks(range(len(label_counts)), label_counts.index, rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train['Labels'].explode()\n",
    "\n",
    "# Get the label counts\n",
    "label_counts = labels.value_counts().sort_index()\n",
    "\n",
    "# Add missing label if it doesn't exist\n",
    "if '12' not in label_counts.index:\n",
    "    label_counts['12'] = 0\n",
    "\n",
    "# Sort labels based on their values\n",
    "label_counts = label_counts.sort_values()\n",
    "\n",
    "# Plot the distribution of labels using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Labels')\n",
    "\n",
    "# Set custom x-tick labels\n",
    "plt.xticks(range(len(label_counts)), label_counts.index, rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['12'], axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, csv, train = True, test = False,full=False):\n",
    "        self.csv = csv # df_train\n",
    "        self.train = train # boolean\n",
    "        self.full = full\n",
    "        self.test = test # boolean\n",
    "\n",
    "        self.all_image_names = self.csv[:]['ImageID']\n",
    "        self.captions = self.csv[:]['Caption']\n",
    "\n",
    "        #self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels', 'Caption'], axis=1))\n",
    "\n",
    "        self.train_ratio = int(0.85 * len(self.csv))\n",
    "        self.valid_ratio = len(self.csv) - self.train_ratio\n",
    "\n",
    "        # set the training data images and labels\n",
    "        if self.full == True:\n",
    "            self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels', 'Caption'], axis=1))\n",
    "            self.image_names = list(self.all_image_names)\n",
    "            self.labels = list(self.all_labels)\n",
    "            print(f\"Number of training images: {self.all_labels.shape[0]}\")\n",
    "            # define the training transforms\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=45),\n",
    "                # transforms.ColorJitter(),\n",
    "                transforms.Normalize(mean, std)\n",
    "\n",
    "            ])\n",
    "        elif self.train == True:\n",
    "            self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels', 'Caption'], axis=1))\n",
    "            print(f\"Number of training images: {self.train_ratio}\")\n",
    "            self.image_names = list(self.all_image_names[:self.train_ratio])\n",
    "            self.labels = list(self.all_labels[:self.train_ratio])\n",
    "            self.captions = list(self.captions[:self.train_ratio])\n",
    "            # define the training transforms\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=45),\n",
    "                # transforms.ColorJitter(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        # set the validation data images and labels\n",
    "        elif self.train == False and self.test == True:\n",
    "            self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels','Caption'], axis=1))\n",
    "            print(f\"Number of validation images: {self.valid_ratio}\")\n",
    "            self.image_names = list(self.all_image_names[-self.valid_ratio:])\n",
    "            self.labels = list(self.all_labels[-self.valid_ratio:])\n",
    "            self.captions = list(self.captions[-self.valid_ratio:])\n",
    "            # define the validation transforms\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "\n",
    "            ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.image_names[index]\n",
    "        image = Image.open(f'data/{name}')\n",
    "        image = self.transform(image)\n",
    "        caption = self.captions[index]\n",
    "        if self.train == True:   #training\n",
    "            targets = self.labels[index]\n",
    "            return {\n",
    "            'image': torch.tensor(image, dtype=torch.float32),\n",
    "            'label': torch.tensor(targets, dtype=torch.float32),\n",
    "            'name' : name,\n",
    "            'caption' : caption,\n",
    "        }\n",
    "        elif self.train == False and self.test == True:  #validation\n",
    "             targets = self.labels[index]\n",
    "             return {\n",
    "            'image': torch.tensor(image, dtype=torch.float32),\n",
    "            'label': torch.tensor(targets, dtype=torch.float32),\n",
    "            'caption' : caption,\n",
    "            'name' : name,\n",
    "        }\n",
    "        elif self.test == True and self.train == False:   #testing\n",
    "            return {\n",
    "            'image': torch.tensor(image, dtype=torch.float32),\n",
    "            'name' : name,\n",
    "            'caption' : caption,\n",
    "        }\n",
    "       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv\n",
    "        self.image_names = self.csv[:]['ImageID']\n",
    "        self.captions = self.csv[:]['Caption']\n",
    "        print(f\"Number of test images: {len(self.csv)}\")\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "            \n",
    "        ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.image_names[index]\n",
    "        image = Image.open(f'data/{name}')\n",
    "        image = self.transform(image)\n",
    "        caption = self.captions[index]\n",
    "        return {\n",
    "        'image': torch.tensor(image, dtype=torch.float32),\n",
    "        'name' : name,\n",
    "        'caption' : caption,\n",
    "        }\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = TrainDataset(\n",
    "    df_train, train=True, test=False\n",
    ")\n",
    "# validation dataset\n",
    "valid_data = TrainDataset(\n",
    "    df_train, train=False, test=True\n",
    ")\n",
    "\n",
    "full_data = TrainDataset(\n",
    "   csv = df_train, full=True\n",
    ")\n",
    "\n",
    "test_data = TestDataset(\n",
    "    csv = df_test\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "# validation data loader\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "full_loader = DataLoader(\n",
    "    full_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data[0]['caption'], valid_data[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(valid_data[0]['image'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(valid_data[0]['image'].permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]['caption'], train_data[0]['name'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data[0]['image'].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[len(test_data) -1]['caption'], test_data[len(test_data) -1]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_data[len(test_data) -1]['image'].permute(1, 2, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for predicting the labels on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data_vision(model, test_loader, device):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['ImageID'] = [\"\" for i in range(len(test_data))]\n",
    "    df_pred['Labels'] = [\"\" for i in range(len(test_data))]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            #images\n",
    "            batch_size = len(data['image'])\n",
    "            images = data['image'].to(device)\n",
    "            pred = model(images)\n",
    "            _, argmax_indices = torch.max(pred, dim=1)\n",
    "            pred[torch.arange(pred.size(0)), argmax_indices] = 1\n",
    "            pred = (pred >= 0.5).type(torch.uint8)\n",
    "            start = i*batch_size\n",
    "            end = min((i+1)*batch_size, len(test_data))\n",
    "            indices = pred.nonzero()\n",
    "            indices[indices[:, 1] > 10, 1] += 1\n",
    "            # print(indices)\n",
    "            indices_by_row = torch.split(indices[:, 1], indices[:, 0].unique(return_counts=True)[1].tolist())\n",
    "            results = [\" \".join([str(idx.item()+1) for idx in row]) for row in indices_by_row]\n",
    "            df_pred[\"ImageID\"][start:end] = data['name']\n",
    "            df_pred[\"Labels\"][start:end] = results\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data_caption(model, test_loader, device, tokenizer, seq_len, word_index):\n",
    "    model.eval()\n",
    "    y_pred = torch.zeros(len(test_data), 18)\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['ImageID'] = [\"\" for i in range(len(test_data))]\n",
    "    df_pred['Labels'] = [\"\" for i in range(len(test_data))]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            batch_size = len(data['caption'])\n",
    "            captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "            captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "            #captions = data['Caption'].to(device)\n",
    "            pred = model(captions)\n",
    "            _, argmax_indices = torch.max(pred, dim=1)\n",
    "            pred[torch.arange(pred.size(0)), argmax_indices] = 1\n",
    "            pred = (pred >= 0.5).type(torch.uint8)\n",
    "            start = i*batch_size\n",
    "            end = min((i+1)*batch_size, len(test_data))\n",
    "            indices = pred.nonzero()\n",
    "            indices[indices[:, 1] > 10, 1] += 1 ### incrementing labels that are bigger than 10, since we eliminated the 12 label\n",
    "            indices_by_row = torch.split(indices[:, 1], indices[:, 0].unique(return_counts=True)[1].tolist())\n",
    "            results = [\" \".join([str(idx.item()+1) for idx in row]) for row in indices_by_row]\n",
    "            df_pred[\"ImageID\"][start:end] = data['name']\n",
    "            df_pred[\"Labels\"][start:end] = results\n",
    "\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data_combined(best_cv_model, best_nlp_model, classifier, test_loader, device, tokenizer, seq_len, word_index):\n",
    "    best_cv_model.eval()\n",
    "    best_nlp_model.eval()\n",
    "    classifier.eval()\n",
    "    y_pred = torch.zeros(len(test_data), 18)\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['ImageID'] = [\"\" for i in range(len(test_data))]\n",
    "    df_pred['Labels'] = [\"\" for i in range(len(test_data))]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            batch_size = len(data['caption'])\n",
    "            print(batch_size)\n",
    "            #captions\n",
    "            captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "            captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "\n",
    "            #images\n",
    "            images = data['image'].to(device)\n",
    "\n",
    "            #captions = data['Caption'].to(device)\n",
    "            #optimizer.zero_grad()\n",
    "            img_out = best_cv_model(images)\n",
    "            nlp_out = best_nlp_model(captions)\n",
    "            concatenating_outs = torch.concat((img_out, nlp_out), 1)\n",
    "            combined_model_pred = classifier(concatenating_outs)\n",
    "            _, argmax_indices = torch.max(combined_model_pred, dim=1)\n",
    "            combined_model_pred[torch.arange(combined_model_pred.size(0)), argmax_indices] = 1\n",
    "            combined_model_pred = (combined_model_pred >= 0.5).type(torch.uint8)\n",
    "            start = i*batch_size\n",
    "            end = min((i+1)*batch_size, len(test_data))\n",
    "            indices = combined_model_pred.nonzero()\n",
    "            indices[indices[:, 1] > 10, 1] += 1 ### incrementing labels that are bigger than 10, since we eliminated the 12 label       \n",
    "            indices_by_row = torch.split(indices[:, 1], indices[:, 0].unique(return_counts=True)[1].tolist())\n",
    "            results = [\" \".join([str(idx.item()+1) for idx in row]) for row in indices_by_row]\n",
    "            df_pred[\"ImageID\"][start:end] = data['name']\n",
    "            df_pred[\"Labels\"][start:end] = results\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision Models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "class AlexNet_1(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        alexnet = AlexNet_model.to(device)\n",
    "        alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features,num_classes)\n",
    "        self.base_model = alexnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                \n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "class AlexNet_2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        alexnet = AlexNet_model.to(device)\n",
    "        alexnet.classifier[4] = nn.Linear(alexnet.classifier[4].in_features,1096)\n",
    "        alexnet.classifier[6] = nn.Linear(1096,num_classes)\n",
    "        self.base_model = alexnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                \n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Use the torchvision's implementation of ResNeXt, but add FC layer for a different number of classes (27) and a Sigmoid instead of a default Softmax.\n",
    "class Resnext50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        resnet = models.resnext50_32x4d(pretrained=True).to(device=device)\n",
    "\n",
    "        self.base_model = resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True).to(device=device)\n",
    "        resnet.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "        self.base_model = resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Resnet18(\n",
       "  (base_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=18, bias=True)\n",
       "  )\n",
       "  (sigm): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cv_model = Resnet18(num_classes=18)\n",
    "best_cv_model = best_cv_model.to(device)\n",
    "best_cv_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_freq = 3\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(best_cv_model.parameters(),weight_decay=weight_decay, lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    }
   ],
   "source": [
    "name_of_model = f'Resnet18_lr={lr}_weight_decay={weight_decay}_dropout=0.5_original_model'\n",
    "os.mkdir(name_of_model)\n",
    "train_losses = np.zeros(num_epochs)\n",
    "valid_losses = np.zeros(num_epochs)\n",
    "f1_scores = np.zeros(num_epochs)\n",
    "initial_lr = optimizer.param_groups[0]['lr']\n",
    "for it in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images = data['image'].to(device)\n",
    "        target = data['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_cv_model(images)\n",
    "        loss = criterion(outputs, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(it)\n",
    "\n",
    "    if ((it % save_freq == 0) or (it == num_epochs - 1)) and it > 0:\n",
    "        checkpoint_path = f'{name_of_model}/{name_of_model}{it}.pth'\n",
    "        checkpoint = {'model': best_cv_model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'epoch': num_epochs}\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "    train_losses[it] = train_loss/len(train_loader)\n",
    "    result = best_cv_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)\n",
    "    valid_losses[it] = result['loss']\n",
    "    f1_scores[it] = result['f1_score']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78634885, 0.78200172, 0.79239979, 0.78760882, 0.79758613,\n",
       "       0.80628721, 0.80980383, 0.80609221, 0.81469663, 0.80045952,\n",
       "       0.79913271, 0.80046595, 0.79616756, 0.80504664, 0.79292275,\n",
       "       0.78190497, 0.78913527, 0.79452367, 0.7843489 , 0.77988475])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8146966329966329"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = predict_test_data_vision(best_cv_model, test_loader=test_loader, device=device, batch_size = batch_size)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(num_epochs),train_losses, label = \"training loss\")\n",
    "plt.plot(np.arange(num_epochs),valid_losses,label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{name_of_model}/learning_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 ={\n",
    "    \"f1_scores\": f1_scores.tolist(),\n",
    "    \"train_losses\": train_losses.tolist(),\n",
    "    \"valid_losses\": valid_losses.tolist()\n",
    "}\n",
    "  \n",
    "# the json file where the output must be stored\n",
    "out_file = open(f\"{name_of_model}/myfile.json\", \"w\")\n",
    "\n",
    "\n",
    "json.dump(dict1, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_model = AlexNet_2(18)\n",
    "checkpoint = torch.load(\"AlexNet_lr=0.0001_weight_decay=0_dropout=0.5/AlexNet_lr=0.0001_weight_decay=0_dropout=0.519.pth\")\n",
    "best_cv_model.load_state_dict(checkpoint['model'])\n",
    "best_cv_model = best_cv_model.to(device)\n",
    "best_cv_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Woman in swim suit holding parasol on sunny day.\n",
       "1        A couple of men riding horses on top of a gree...\n",
       "2        They are brave for riding in the jungle on tho...\n",
       "3        a black and silver clock tower at an intersect...\n",
       "4         A train coming to a stop on the tracks out side.\n",
       "                               ...                        \n",
       "39995    A group of men riding surfboards riding a mass...\n",
       "39996    A motorcycle parked next to a car in a parking...\n",
       "39997              a little boy that is playing with a wii\n",
       "39998    group of kids play Frisbee golf in the middle ...\n",
       "39999     A man in a gray jacket standing next to a woman.\n",
       "Name: Caption, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.drop(columns = 'Caption').join(df_train['Caption'].str.replace('\\\"', ''))\n",
    "df_test = df_test.drop(columns = 'Caption').join(df_test['Caption'].str.replace('\\\"', ''))\n",
    "whole_sentences = pd.concat([df_train['Caption'], df_test['Caption']], axis=0, ignore_index=True)\n",
    "whole_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yaraslauivashynka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yaraslauivashynka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yaraslauivashynka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw\n",
    "# stop_words = sw.words()\n",
    "STOPWORDS = set(sw.words('english'))\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_split(input_string): # string to pure word\n",
    "    splits = []\n",
    "    for sent in input_string:\n",
    "        # print(text)\n",
    "        sent = sent.lower() # lowercase\n",
    "        sent = re.sub(r'[^A-Za-z]+', ' ', sent) # remove symbols / digits\n",
    "        # text = re.sub(r'[0-9]','',text)\n",
    "        orig_sent = []\n",
    "        for item in sent.split():\n",
    "            if item not in STOPWORDS: # remove stopword\n",
    "                orig_sent.append(item)\n",
    "\n",
    "        lem = [lemmatizer.lemmatize(sent) for sent in orig_sent] # lammatisation\n",
    "\n",
    "        # token = [word_tokenize(word) for word in text_le]\n",
    "        splits.append(lem)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max(len(s) for s in sent_split(whole_sentences))\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "word_embedding_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233353"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_sentences = sent_split(whole_sentences)\n",
    "word_list = []\n",
    "for sent in splitted_sentences:\n",
    "    for word in sent:\n",
    "        word_list.append(word)\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for sent in splitted_sentences:\n",
    "    for word in sent:\n",
    "        vocab_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17480469  0.17871094  0.09082031 ...  0.07568359 -0.11181641\n",
      "  -0.0625    ]\n",
      " ...\n",
      " [-0.08544922 -0.10253906 -0.48632812 ...  0.02075195  0.08496094\n",
      "   0.00061035]\n",
      " [-0.07421875 -0.10205078  0.20117188 ... -0.25390625  0.06054688\n",
      "  -0.21289062]\n",
      " [-0.04248047  0.16015625 -0.2265625  ...  0.18945312 -0.00692749\n",
      "   0.1328125 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2218, 300)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Embedding lookup table\n",
    "import numpy as np\n",
    "emb_dim = word_embedding_model.vector_size\n",
    "word_set = set()\n",
    "min_freq = 5\n",
    "from collections import Counter\n",
    "c = Counter(vocab_list)\n",
    "for i in c:\n",
    "    if c[i] >= min_freq:\n",
    "        word_set.add(i)\n",
    "word_set.add('[PAD]')\n",
    "word_set.add('[UNKOWN]')\n",
    "word_list=list(word_set)\n",
    "word_list.sort()\n",
    "emb_table = []\n",
    "word_index = {}\n",
    "emb_table = []\n",
    "for i, word in enumerate(word_list):\n",
    "    word_index[word] = i\n",
    "    if word in word_embedding_model:\n",
    "        emb_table.append(word_embedding_model[word])\n",
    "    else:\n",
    "        emb_table.append([0]*emb_dim)\n",
    "emb_table = np.array(emb_table)\n",
    "    \n",
    "print(emb_table)\n",
    "emb_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def tokenize(self, sentences, seq_len, word_index):\n",
    "        sentences = sent_split(sentences)\n",
    "        sent_encoded = []\n",
    "        for sent in sentences:\n",
    "            temp_encoded = [word_index[word] if word in word_index else word_index['[UNKOWN]'] for word in sent]\n",
    "            if len(temp_encoded) < seq_len:\n",
    "                temp_encoded += [word_index['[PAD]']] * (seq_len - len(temp_encoded))\n",
    "            else:\n",
    "                temp_encoded = temp_encoded[:seq_len]\n",
    "            sent_encoded.append(temp_encoded)\n",
    "        return sent_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2218, 300)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = emb_table.shape[0]\n",
    "emb_dim = emb_table.shape[1]\n",
    "vocab_size, emb_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "n_hidden = 300\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "seq_len = max_seq_len\n",
    "\n",
    "class Bi_LSTM_Emb(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super(Bi_LSTM_Emb, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
    "        self.emb.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n",
    "        self.linear = nn.Linear(n_hidden*2, n_classes)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)        \n",
    "        lstm_out, (h_n,c_n) = self.lstm(x)\n",
    "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
    "        z = self.linear(hidden_out)\n",
    "        return self.sigm(z)\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device):\n",
    "            self.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                for data in loader:\n",
    "                    captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "                    captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "                    target = data['label'].to(device)\n",
    "                    outputs = self(captions)\n",
    "                    loss = criterion(outputs, target)\n",
    "                    val_loss += loss.item()\n",
    "                    outputs = outputs.cpu().numpy()\n",
    "                    argmax_indices = np.argmax(outputs, axis=1)\n",
    "                    outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                    predicted = np.round(outputs)\n",
    "                    y_pred.extend(predicted)\n",
    "                    y_true.extend(target.cpu().numpy())\n",
    "                y_pred = np.array(y_pred)\n",
    "                y_true = np.array(y_true)\n",
    "                res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "            return {'f1_score' : res, 'loss' : val_loss/len(loader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "n_hidden = 300\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "seq_len = max_seq_len\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "  def __init__(self, n_class):\n",
    "    super(LSTM, self).__init__()\n",
    "    self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "    self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
    "    self.emb.weight.requires_grad = False\n",
    "    self.lstm = nn.LSTM(emb_dim, n_hidden, num_layers=2, batch_first =True, dropout=0.5)\n",
    "    self.linear = nn.Linear(n_hidden,n_class)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "  \n",
    "  def forward(self, input):\n",
    "    input = self.emb(input)\n",
    "    input,_ = self.lstm(input)\n",
    "    input = self.linear(input[:,-1,:])\n",
    "    return self.sigmoid(input)\n",
    "  \n",
    "  def calculate_f1_score_and_loss(self, loader, criterion, device):\n",
    "    self.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for data in loader:\n",
    "            captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "            captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "            target = data['label'].to(device)\n",
    "            outputs = self(captions)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            argmax_indices = np.argmax(outputs, axis=1)\n",
    "            outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "            predicted = np.round(outputs)\n",
    "            y_pred.extend(predicted)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_true = np.array(y_true)\n",
    "        res = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    return {'f1_score' : res, 'loss' : val_loss/len(loader)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bi_LSTM_Emb(\n",
       "  (emb): Embedding(2218, 300)\n",
       "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=600, out_features=18, bias=True)\n",
       "  (sigm): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_nlp_model = Bi_LSTM_Emb(18)\n",
    "best_nlp_model = best_nlp_model.to(device)\n",
    "best_nlp_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "save_freq = 3\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(best_nlp_model.parameters(), weight_decay=weight_decay, lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_model = f'BiLSTM_lr={lr}_weight_decay={weight_decay}_dropout=0.5'\n",
    "os.mkdir(name_of_model)\n",
    "train_losses = torch.zeros(num_epochs)\n",
    "val_losses = torch.zeros(num_epochs)\n",
    "f1_scores = torch.zeros(num_epochs)\n",
    "for it in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    best_nlp_model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "        captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "        target = data['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_nlp_model(captions)\n",
    "        loss = criterion(outputs, target)\n",
    "        train_loss+=loss.item()\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if ((it % save_freq == 0) or (it == num_epochs - 1)) and it > 0:\n",
    "        checkpoint_path = f'{name_of_model}/{name_of_model}{it}.pth'\n",
    "        checkpoint = {'model': best_cv_model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'epoch': num_epochs}\n",
    "        torch.save(checkpoint, checkpoint_path)        \n",
    "    print(it)\n",
    "    train_losses[it] = train_loss/len(train_loader)\n",
    "    result = best_nlp_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)\n",
    "    val_losses[it] = result['loss']\n",
    "    f1_scores[it] = result['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2237, 0.2004, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005,\n",
       "        0.2006, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005, 0.2006,\n",
       "        0.2005, 0.2005])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(num_epochs),train_losses, label = \"training loss\")\n",
    "plt.plot(np.arange(num_epochs),valid_losses,label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{name_of_model}/learning_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 ={\n",
    "    \"f1_scores\": f1_scores.tolist(),\n",
    "    \"train_losses\": train_losses.tolist(),\n",
    "    \"valid_losses\": valid_losses.tolist()\n",
    "}\n",
    "  \n",
    "# the json file where the output must be stored\n",
    "out_file = open(f\"{name_of_model}/myfile.json\", \"w\")\n",
    "json.dump(dict1, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINED MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine the NLP and classification model we need to concatenate the features and run a third classifier on this concatenated features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the third classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score_and_loss(model, best_nlp_model, best_cv_model, loader, criterion, seq_len, word_index, tokenizer, device=None):\n",
    "        model.eval()\n",
    "        best_nlp_model.eval()\n",
    "        best_cv_model.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "\n",
    "                captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "                captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "\n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "\n",
    "                image_outs = best_cv_model(images)\n",
    "                text_outs = best_nlp_model(captions)\n",
    "\n",
    "                concatenating_outs = torch.concat((image_outs, text_outs), 1)\n",
    "                combined_outs = classifier(concatenating_outs)\n",
    "\n",
    "                loss = criterion(combined_outs, targets.type(torch.float))\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = combined_outs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 18\n",
    "\n",
    "best_cv_model = Resnext50(num_classes=n_labels)\n",
    "best_nlp_model = Bi_LSTM_Emb(n_classes=n_labels)\n",
    "checkpoint = torch.load(\"ResNet4.pth\")\n",
    "# Load the model state dictionary from the checkpoint\n",
    "best_cv_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "checkpoint = torch.load(\"BiLSTM3.pth\")\n",
    "# Load the model state dictionary from the checkpoint\n",
    "best_nlp_model.load_state_dict(checkpoint['model'])\n",
    "# Define Classifier\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=36, out_features=3100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=3100, out_features=n_labels),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "\n",
    "# Switch model to GPU.\n",
    "best_cv_model = best_cv_model.to(device)\n",
    "best_nlp_model = best_nlp_model.to(device)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device=device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_losses = torch.zeros(num_epochs)\n",
    "val_losses = torch.zeros(num_epochs)\n",
    "f1_scores = torch.zeros(num_epochs)\n",
    "name_of_model = \"combined_model\"\n",
    "saving_freq = 3\n",
    "os.mkdir(name_of_model)\n",
    "for epoch in range(epochs):\n",
    "    classifier.train()\n",
    "    batch_loss_values = []\n",
    "    train_loss=0\n",
    "    for data in train_loader:\n",
    "        best_cv_model.train()\n",
    "        best_nlp_model.train()\n",
    "\n",
    "        targets = data['label'].to(device)\n",
    "        images = data['image'].to(device)\n",
    "       \n",
    "        captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "        captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        img_out = best_cv_model(images)\n",
    "        nlp_out = best_nlp_model(captions)\n",
    "        concatenating_outs = torch.concat((img_out, nlp_out), 1)\n",
    "\n",
    "        combined_model = classifier(concatenating_outs)\n",
    "        \n",
    "        #loss and backward pass\n",
    "        loss = criterion(combined_model, targets.type(torch.float))\n",
    "        train_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(loss.item())\n",
    "        # bl_value = loss.item()\n",
    "        # print(bl_value)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    if ((epoch % saving_freq == 0) or (epoch == epochs - 1)) and epoch > 0:\n",
    "        checkpoint_path = f'{name_of_model}/{name_of_model}{epoch}.pth'\n",
    "        checkpoint = {'model': classifier.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'epoch': num_epochs}\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    print(f\"epoch {epoch}\")\n",
    "    train_losses[it] = train_loss/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(num_epochs),train_losses, label = \"training loss\")\n",
    "plt.plot(np.arange(num_epochs),val_losses,label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
